# 1236. Web Crawler:

<img width="721" alt="image" src="https://user-images.githubusercontent.com/35987583/168226423-0cae0958-8455-42a3-950b-92241aeb3949.png">
<img width="711" alt="image" src="https://user-images.githubusercontent.com/35987583/168226454-950ad9a8-3f57-4cf0-b57e-bfbb16e100ab.png">
<img width="689" alt="image" src="https://user-images.githubusercontent.com/35987583/168226500-7843564f-c11e-4c41-8285-bc6e0fc0f866.png">
<img width="710" alt="image" src="https://user-images.githubusercontent.com/35987583/168226551-c5fd8a3f-f6b8-4f87-a4b4-5fab5d7ec77d.png">
<img width="723" alt="image" src="https://user-images.githubusercontent.com/35987583/168226596-1848fd77-44ca-4c48-b82d-9d85f7b7ae27.png">


BFS ----> Queue

```python
# """
# This is HtmlParser's API interface.
# You should not implement it, or speculate about its implementation
# """
#class HtmlParser(object):
#    def getUrls(self, url):
#        """
#        :type url: str
#        :rtype List[str]
#        """
import collections
class Solution:
    def crawl(self, startUrl: str, htmlParser: 'HtmlParser') -> List[str]:
        if startUrl is None or len(startUrl) == 0:
            return []
        
        visited = set()
        visited.add(startUrl)
        domain = startUrl.split('://')[1].split('/')[0]
        result = [startUrl]                
        nextLevel = []
        
        queue = collections.deque([startUrl])
        
        while queue:
            for _ in range(len(queue)):
                url = queue.popleft()
                childUrls = htmlParser.getUrls(url)
                for c in childUrls:
                    if c.split('://')[1].split('/')[0] == domain and c not in visited:
                        result.append(c)
                        queue.append(c)
                        visited.add(c)
        return result
```

# Time: O(N)
# space: O(H)
